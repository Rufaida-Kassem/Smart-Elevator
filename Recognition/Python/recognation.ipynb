{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lbp_hist(grayscale_img):\n",
    "    hist = np.zeros(256)\n",
    "    for i in range (1, grayscale_img.shape[0] - 1):\n",
    "        for j in range (1, grayscale_img.shape[1] - 1):\n",
    "            b = 0b00000000\n",
    "            if(grayscale_img[i - 1][j- 1] > grayscale_img[i][j]):\n",
    "                b = b | 0b10000000\n",
    "            if(grayscale_img[i - 1][j] > grayscale_img[i][j]):\n",
    "                b = b | 0b01000000\n",
    "            if(grayscale_img[i - 1][j+ 1] > grayscale_img[i][j]):\n",
    "                b = b | 0b00100000\n",
    "            if(grayscale_img[i][j + 1] > grayscale_img[i][j]):\n",
    "                b = b | 0b00010000\n",
    "            if(grayscale_img[i + 1][j + 1] > grayscale_img[i][j]):\n",
    "                b = b | 0b00001000\n",
    "            if(grayscale_img[i + 1][j] > grayscale_img[i][j]):\n",
    "                b = b | 0b00000100\n",
    "            if(grayscale_img[i + 1][j- 1] > grayscale_img[i][j]):\n",
    "                b = b | 0b00000010\n",
    "            if(grayscale_img[i][j- 1] > grayscale_img[i][j]):\n",
    "                b = b | 0b00000001\n",
    "            hist[b] = hist[b] + 1\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lbph(images):\n",
    "    noOfPersons = images.shape[0]\n",
    "    hist = []  \n",
    "    for k in range(noOfPersons):\n",
    "        for face in images[k]:\n",
    "            face_resize = cv2.resize(face, (256, 256))  \n",
    "            hist_lbp = get_lbp_hist(face_resize)  \n",
    "            hist.append(hist_lbp)\n",
    "    return hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lbph(input_image,recognizer,residentsNames):\n",
    "    d1=recognizer.shape[0]\n",
    "    input_hist = get_lbp_hist(cv2.resize(input_image, (256, 256)))\n",
    "    distance = 0\n",
    "    thresh = 2000000\n",
    "    index = 0\n",
    "    eps = 1e-10\n",
    "    for i in range(d1):\n",
    "        distance = 0.005 * np.sum([((a - b) ** 2) / (a + b + eps)\n",
    "        for (a, b) in zip(recognizer[i,:], input_hist)])\n",
    "        if distance<thresh:\n",
    "            index=i\n",
    "            thresh=distance\n",
    "    return (residentsNames[index],thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_faces_residentName(gray):\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades +  \"haarcascade_frontalface_default.xml\")\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.3,\n",
    "        minNeighbors=4,\n",
    "        minSize=(60, 60)\n",
    "    )\n",
    "    face_images=[]\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(gray, (x, y-15), (x+w, y+h), (0, 255, 0), 2)\n",
    "        roi_color = gray[y-15:y + h, x:x + w]\n",
    "        \n",
    "        face_images.append(roi_color)\n",
    "    return face_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_lbph_residentName():\n",
    "    (images, residentsNames, names, id) = ([], [], {}, 0)\n",
    "    for (subdirs, dirs, files) in os.walk('../TrainingImages'):\n",
    "        for subdir in dirs:\n",
    "            names[id] = subdir\n",
    "            subjectpath = os.path.join('../TrainingImages', subdir)\n",
    "            num=0\n",
    "            for filename in os.listdir(subjectpath):\n",
    "                path = subjectpath + '/' + filename\n",
    "                lable = subdir\n",
    "                images.append(cv2.imread(path, 0))\n",
    "                residentsNames.append(lable)\n",
    "                num+=1\n",
    "\n",
    "            id += 1\n",
    "    (im_width, im_height) = (68, 68)\n",
    "    (images_arr, residentsNames_arr) = [np.array(lis) for lis in [images, residentsNames]]\n",
    "    s1 = images_arr.shape[0]\n",
    "    total_faces=[]\n",
    "    for i in range(s1):\n",
    "        faces = extract_faces_residentName(images_arr[i])   \n",
    "        total_faces.append(faces)\n",
    "    (faces_arr, residentsNames_arr) = [np.array(lis) for lis in [total_faces, residentsNames]]\n",
    "    trained_face_recognizer=train_lbph(faces_arr) \n",
    "    print('finished training')\n",
    "    np.save('trained.npy',trained_face_recognizer)\n",
    "\n",
    "    return residentsNames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rufai\\AppData\\Local\\Temp\\ipykernel_21096\\1995458314.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  (images_arr, residentsNames_arr) = [np.array(lis) for lis in [images, residentsNames]]\n",
      "C:\\Users\\Rufai\\AppData\\Local\\Temp\\ipykernel_21096\\1995458314.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  (faces_arr, residentsNames_arr) = [np.array(lis) for lis in [total_faces, residentsNames]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training\n"
     ]
    }
   ],
   "source": [
    "residentsNames=training_lbph_residentName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(imgfile):\n",
    "    np_load_old = np.load\n",
    "    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "    trained_face_recognizer=np.load('trained.npy')\n",
    "    np.load = np_load_old\n",
    "    myimage=io.imread('../TestImages/'+imgfile)\n",
    "    gray = cv2.cvtColor(myimage, cv2.COLOR_BGR2GRAY)\n",
    "    faces=extract_faces_residentName(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        prediction=predict_lbph(face,trained_face_recognizer,residentsNames)\n",
    "        if (prediction[1])<3000:\n",
    "            print(\"Hello \" + prediction[0] + \", going to the target floor now :)\")\n",
    "        else:\n",
    "            print(\"Probably, you are a visitor. Welcome to our building :)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello mo salah, going to the target floor now :)\n"
     ]
    }
   ],
   "source": [
    "test('mbappe.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70d459914d9a86456d6cb0e34fc8190790cc0df25ff27e38f763783165fe5050"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
